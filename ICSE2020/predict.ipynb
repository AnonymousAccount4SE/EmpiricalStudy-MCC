{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2964074\n",
      "2964074\n",
      "call resolver void debugger breakpoint hit reply step big integer breakpoint address get program counter reply get thread id reply get register values count hit reply get thread id breakpoint address target get debugger single step debug exception e e print stack trace\n",
      "ssss breakpoint hit eeee\n",
      "(2964074, 172)\n",
      "(2964074, 6)\n",
      "1\n",
      "2\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    2   15   30    2   44   30\n",
      "    2   44   30    2  244    1  128    1  603   44   30    2    2   15\n",
      "    5    1  603    5  603   86    8  490   44   30    2   44   30    2\n",
      "  244    1  186   44   30    2 1709 2532    1  186  603  603 1647  186\n",
      "   44   30    2  186  186 1647   44   30    2   30    2   30    2   30\n",
      "    2    2  388 1647]\n",
      "string to builder string append builder string append builder string address get target get edge append builder string string to type get edge type edge of value enum append builder string append builder string address get block append builder string edges outgoing get block edge edge reil block append builder string block block reil append builder string builder string builder string builder string string graph reil\n",
      "reil graph string string builder string builder string builder string builder append reil block block string builder append block reil edge edge block get outgoing edges string builder append block get address string builder append string builder append enum value of edge type edge get type to string string builder append edge get target get address string builder append string builder append string builder to string\n",
      "[ 1  6 11  2  0  0]\n",
      "ssss to string eeee\n",
      "ssss to string eeee\n",
      "(2964074, 5)\n",
      "(2964074, 5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "result_path = '/home1/likejun/BadNameDetectionICSE2020/result'\n",
    "data_path = '/home1/likejun/BadNameDetectionICSE2020/data/'\n",
    "method_name_path = data_path + 'ParsedMethodNameTokens_1.txt'\n",
    "context_path = data_path + 'ParsedMethodContextTokens_1.txt'\n",
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "\n",
    "\n",
    "def load_data(src=True, start=\"\", end=\"\"):\n",
    "    results = []\n",
    "    if src:\n",
    "        path = context_path\n",
    "        with open(path) as file:\n",
    "            texts = [start + line.strip() + end for line in file]\n",
    "            results.extend(texts)\n",
    "    else:\n",
    "        path = method_name_path\n",
    "        with open(path) as file:\n",
    "            texts = [start + line.strip() + end for line in file]\n",
    "            results.extend(texts)\n",
    "    return results\n",
    "\n",
    "\n",
    "data_src = load_data(src=True)\n",
    "data_dest = load_data(src=False, start=mark_start, end=mark_end)\n",
    "\n",
    "print(len(data_src))\n",
    "print(len(data_dest))\n",
    "\n",
    "idx = 10000\n",
    "print(data_src[idx])\n",
    "print(data_dest[idx])\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, padding, reverse=False, num_words=None):\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words, filters='\"#$.?@\\\\^_`~\\t\\n', split=' ')\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(), self.word_index.keys()))\n",
    "\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "\n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)\n",
    "\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)\n",
    "\n",
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)\n",
    "\n",
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "print(token_start)\n",
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "print(token_end)\n",
    "\n",
    "idx = 0\n",
    "print(tokens_src[idx])\n",
    "print(tokenizer_src.tokens_to_string(tokens_src[idx]))\n",
    "print(data_src[idx])\n",
    "\n",
    "print(tokens_dest[idx])\n",
    "print(tokenizer_dest.tokens_to_string(tokens_dest[idx]))\n",
    "print(data_dest[idx])\n",
    "\n",
    "encoder_input_data = tokens_src\n",
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "print(decoder_input_data.shape)\n",
    "\n",
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "print(decoder_output_data.shape)\n",
    "\n",
    "idx = 22\n",
    "decoder_input_data[idx]\n",
    "decoder_output_data[idx]\n",
    "tokenizer_dest.tokens_to_string(decoder_input_data[idx])\n",
    "tokenizer_dest.tokens_to_string(decoder_output_data[idx])\n",
    "\n",
    "encoder_input = Input(shape=(None,), name='encoder_input')\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='encoder_embedding')\n",
    "\n",
    "state_size = 512\n",
    "\n",
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)\n",
    "\n",
    "\n",
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "\n",
    "    return encoder_output\n",
    "\n",
    "\n",
    "encoder_output = connect_encoder()\n",
    "\n",
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')\n",
    "\n",
    "decoder_input = Input(shape=(None,), name='decoder_input')\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')\n",
    "\n",
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)\n",
    "\n",
    "decoder_dense = Dense(num_words,\n",
    "                      activation='linear',\n",
    "                      name='decoder_output')\n",
    "\n",
    "\n",
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "\n",
    "    return decoder_output\n",
    "\n",
    "\n",
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])\n",
    "\n",
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])\n",
    "\n",
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])\n",
    "\n",
    "\n",
    "# model_train.compile(optimizer=optimizer,\n",
    "#                     loss='sparse_categorical_crossentropy')\n",
    "\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between y_true and y_pred.\n",
    "\n",
    "    y_true is a 2-rank tensor with the desired output.\n",
    "    The shape is [batch_size, sequence_length] and it\n",
    "    contains sequences of integer-tokens.\n",
    "\n",
    "    y_pred is the decoder's output which is a 3-rank tensor\n",
    "    with shape [batch_size, sequence_length, num_words]\n",
    "    so that for each sequence in the batch there is a one-hot\n",
    "    encoded array of length num_words.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the loss. This outputs a\n",
    "    # 2-rank tensor of shape [batch_size, sequence_length]\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire 2-rank tensor, we reduce it\n",
    "    # to a single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean\n",
    "\n",
    "\n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "\n",
    "model_train.compile(optimizer=optimizer,\n",
    "                    loss=sparse_cross_entropy)\n",
    "\n",
    "path_checkpoint = '14m_fse19_return_paras_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./14m_fse19_return_paras_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 128)    3840000     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru1 (GRU)              (None, None, 512)    986112      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru2 (GRU)              (None, None, 512)    1575936     encoder_gru1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 128)    3840000     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru3 (GRU)              (None, 512)          1575936     encoder_gru2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru1 (GRU)              (None, None, 512)    986112      decoder_embedding[0][0]          \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru2 (GRU)              (None, None, 512)    1575936     decoder_gru1[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru3 (GRU)              (None, None, 512)    1575936     decoder_gru2[0][0]               \n",
      "                                                                 encoder_gru3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 30000)  15390000    decoder_gru3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 31,345,968\n",
      "Trainable params: 31,345,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_train.load_weights(result_path + \"14m_fse19_v11.hdf5\") #TrainingDataAllTokens + NegativeItems\n",
    "# model_train.load_weights(result_path + \"14m_fse19_v10.hdf5\") #TrainingDataLT94Tokens\n",
    "model_train.load_weights(result_path + \"14m_fse19_v9.hdf5\") #TrainingDataAllTokens\n",
    "model_train.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " add eeee\n"
     ]
    }
   ],
   "source": [
    "def trans(input_text):\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        x_data = \\\n",
    "            {\n",
    "                'decoder_initial_state': initial_state,\n",
    "                'decoder_input': decoder_input_data\n",
    "            }\n",
    "\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "\n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "\n",
    "    return output_text\n",
    "\n",
    "\n",
    "print(trans(input_text=data_src[idx]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174944\n",
      "154.95821046829224\n",
      "152.38123965263367\n",
      "148.2330687046051\n",
      "150.30725622177124\n",
      "149.3212730884552\n",
      "143.06449341773987\n",
      "148.83353757858276\n",
      "147.36180591583252\n",
      "147.3600218296051\n",
      "149.93777132034302\n",
      "145.3573019504547\n",
      "156.3977484703064\n",
      "156.72824382781982\n",
      "149.26941895484924\n",
      "156.6216962337494\n",
      "157.04451942443848\n",
      "155.69642806053162\n",
      "153.31396222114563\n",
      "155.64004921913147\n",
      "150.0942883491516\n",
      "144.59273838996887\n",
      "145.64209055900574\n",
      "138.30864000320435\n",
      "144.34853506088257\n",
      "150.1441740989685\n",
      "148.0886402130127\n",
      "149.35336637496948\n",
      "155.14512991905212\n",
      "151.85818767547607\n",
      "149.4592661857605\n",
      "145.37200617790222\n",
      "141.30109310150146\n",
      "132.75192070007324\n",
      "141.29074358940125\n",
      "143.74160361289978\n",
      "144.10642266273499\n",
      "132.3285253047943\n",
      "144.394366979599\n",
      "154.7309215068817\n",
      "146.8138563632965\n",
      "141.64062595367432\n",
      "145.3820059299469\n",
      "150.12429070472717\n",
      "146.4537718296051\n",
      "157.56516981124878\n",
      "156.88175988197327\n",
      "153.9599528312683\n",
      "152.94550204277039\n",
      "151.6874921321869\n",
      "143.14954280853271\n",
      "158.46457529067993\n",
      "150.01848006248474\n",
      "151.64932250976562\n",
      "151.30419993400574\n",
      "150.74734258651733\n",
      "151.78401517868042\n",
      "159.2521631717682\n",
      "147.39004921913147\n",
      "152.85852217674255\n",
      "160.1965868473053\n",
      "149.34492349624634\n",
      "156.13297176361084\n",
      "162.00219559669495\n",
      "162.42453503608704\n",
      "159.32073879241943\n",
      "160.9564516544342\n",
      "143.21875715255737\n",
      "141.78256487846375\n",
      "161.50720167160034\n",
      "151.07952237129211\n",
      "150.54368662834167\n",
      "157.22405219078064\n",
      "149.38025450706482\n",
      "150.64617443084717\n",
      "142.37297320365906\n",
      "150.34723210334778\n",
      "146.96644186973572\n",
      "152.68013787269592\n",
      "148.17891573905945\n",
      "148.61858081817627\n",
      "155.11369967460632\n",
      "154.36209630966187\n",
      "149.57076144218445\n",
      "153.33505773544312\n",
      "148.6005573272705\n",
      "151.70761060714722\n",
      "155.42175889015198\n",
      "147.74005222320557\n",
      "143.40735292434692\n",
      "156.69420647621155\n",
      "149.78303575515747\n",
      "154.31188774108887\n",
      "156.60170459747314\n",
      "151.51750540733337\n",
      "151.4582257270813\n",
      "152.27498698234558\n",
      "145.3561987876892\n",
      "144.4960777759552\n",
      "155.82632422447205\n",
      "155.1994912624359\n",
      "149.715966463089\n",
      "147.255047082901\n",
      "151.617258310318\n",
      "144.80948543548584\n",
      "143.96302843093872\n",
      "150.61452794075012\n",
      "139.38330483436584\n",
      "148.5068244934082\n",
      "152.65794229507446\n",
      "150.98482537269592\n",
      "149.0430474281311\n",
      "155.3362898826599\n",
      "146.47921228408813\n",
      "148.7638349533081\n",
      "154.38562536239624\n",
      "148.803320646286\n",
      "153.04400038719177\n",
      "133.90997958183289\n",
      "148.15518999099731\n",
      "142.43252515792847\n",
      "155.96097016334534\n",
      "138.15107250213623\n",
      "133.1909580230713\n",
      "145.2365186214447\n",
      "153.87415885925293\n",
      "146.10887718200684\n",
      "139.57687997817993\n",
      "144.27424883842468\n",
      "146.77263069152832\n",
      "146.30139183998108\n",
      "145.05983757972717\n",
      "149.62854671478271\n",
      "148.75885796546936\n",
      "151.46561312675476\n",
      "148.0130271911621\n",
      "144.62471294403076\n",
      "159.7284870147705\n",
      "149.47025084495544\n",
      "149.58169746398926\n",
      "157.72583413124084\n",
      "143.55421233177185\n",
      "142.71809220314026\n",
      "143.33823990821838\n",
      "143.99367022514343\n",
      "142.9006016254425\n",
      "142.14096593856812\n",
      "151.56865119934082\n",
      "140.80025577545166\n",
      "146.97746181488037\n",
      "155.46000742912292\n",
      "148.08368158340454\n",
      "142.5568015575409\n",
      "148.83067679405212\n",
      "144.4053225517273\n",
      "145.58129477500916\n",
      "153.9671745300293\n",
      "149.32795548439026\n",
      "146.3793933391571\n",
      "149.9803023338318\n",
      "150.25850129127502\n",
      "152.85948061943054\n",
      "151.33954548835754\n",
      "152.07818365097046\n",
      "145.92444515228271\n",
      "152.6540195941925\n",
      "159.53968620300293\n",
      "150.97409915924072\n",
      "153.45239853858948\n",
      "156.05765175819397\n",
      "148.8010106086731\n",
      "152.30572390556335\n",
      "148.23721170425415\n",
      "150.63290572166443\n",
      "148.64789652824402\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_noreal_more_46_0228/consistent/'\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_noreal_more_46_0228/inconsistent/'\n",
    "# test_path = '/home1/likejun/BadNameDetectionICSE2020/test_real_more_46_0228/inconsistent/'\n",
    "test_path = '/home1/likejun/BadNameDetectionICSE2020/test_real_more_46_0228/consistent/'\n",
    "\n",
    "all_srcs_test = []\n",
    "\n",
    "path = test_path + 'parsedMethodContextTokens.txt'\n",
    "with open(path) as file:\n",
    "    texts = [line.strip() for line in file]\n",
    "    all_srcs_test.extend(texts)\n",
    "print(len(all_srcs_test))\n",
    "name_list = []\n",
    "start = time.time()\n",
    "with open(test_path + 'result_TrainingDataAllTokens.txt', 'w') as f:\n",
    "    for body in all_srcs_test:\n",
    "        name = trans(input_text=body)\n",
    "        f.write(name.replace(\" eeee\", \"\") + '\\n')\n",
    "        name_list.append(1)\n",
    "        if len(name_list) % 1000 == 0:\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "            start = end"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}